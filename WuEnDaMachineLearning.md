#吴恩达机器学习笔记
1. 机器学习通用定义：
	a. 假设用P来评估计算机程序在某任务类T上的性能，若一个程序利用经验E在任务T上获得了性能改善，则我们就说关于T和P，该程序对E进行了学习；
	b. 机器学习中流行的任务类T包括：
		i. 分类：基于特征将实例分为某一类；
		ii. 回归：基于实例的其他特征预测该实例的数值型目标特征；
		iii. 聚类：基于实例的特征实现实例的分组，从而让组内成员比组间成员更为相似；
		iv. 异常检测：寻找与其他样本或者组内实例有很大区别的实例；
	c. 机器学习算法：
		i. 监督学习：
		ii. 无监督学习：
2. Logistic Regression:  want 0 <= h(x) <= 1;
3. Logistic function: h(x) = 1 / (1 + e^ (-x))；
	a. 对于给定输入x，h(x)代表了当输入为x时，h(x)==1的估计概率；
4. 逻辑回归的代价函数：
5. 假设函数： ho(x): ho(x) = o0 + o1*x1 + o2*x2 + o3*x1^2 + o4*x2^2 + ...;
6. 线性回归的代价函数： J(o) 
7. 代价函数的分析：——》凸函数与非凸函数；
8. 线性回归代价函数的优化：
	a. 梯度下降算法；
	b. 共轭梯度下降算法；
	c. BFGS；
	d. L-BFGS；
	e. 以上算法优点，可以快速的得到下降速率a;
9. 决策边界：属于假设函数的一个属性，是由假设函数的参数集决定的，与数据集（训练集）无关;
10. 多类别多分类问题：
	a. 逻辑回归分类器；——》二元回归问题
11. 过度拟合问题：
	a. 概念：高方差
	b. 泛化：新样本能够适用回归模型的能力；
	c. 相关概念 ：
		i. 欠拟合，高偏差
	d. 解决过拟合的办法：
		i. 选择合适的特征量，减少特征量个数；——》模型选择算法；
		ii. 保持特征量个数，但是降低特征量的阶数；——》正则化算法；
		
	e. 正则化思路：
		i. 简化假设函数；
		ii. 正则化线性回归函数
		iii. 正则化参数：主要用来平衡使用高阶项来尽可能拟合训练集数据和尽量减小高阶项对代价函数的影响；
12. 改进梯度优化算法：
	a. 正则化逻辑回归函数的梯度下降函数；
13. 非线性分类器：
14. 神经网络：（非线性假设函数）
	a. 特征量比较多；
	b. 背景知识：
		i. 模仿人类的大脑；
		ii. 人的大脑皮层能接收光、声音以及触觉；
15. 简单的神经网络模型：
	a. 偏置单元：x0；
	b. 激励函数：s(x) = 1 /( 1 + e^(-x));
	c. 第一层，又称输入层；
	d. 隐藏层可能有多个；
	e. 第3层，又称输出层；
16. 拟合神经网络参数：
	a. 二元分类问题；
		i. 代价函数：逻辑回归函数；
		ii. 最小化代价函数的算法：反向传播算法
		iii. 使用反向传播计算代价函数的导数；
	b. 多元分类问题：
17. 神经网络根据输入和前向传播算法计算出输出；
18. 神经网络的代价函数：
	a. 基于神经网络计算出的y与真实向量y'的均方差；
	b. 如何确保使用反向传播算法得出的使代价函数最小的网络参数——Wij（使用梯度递减方式）是可行的？
		i. 梯度检测；
	c. 此外为了得到一个较好的学习算法，需要对W0进行随机初始化：
		i. 一般初始化为一个接近于0的，且在[-epslon, +epslon]区间的随机实数；
19. 机器学习诊断法：
	a. 提高机器学习算法目的；
	b. 评价学习算法；
20. 模型选择：
	a. 数据留出法——数据集分为训练集和测试集（比例一般7: 3）；
		i. 问题：测试集即用来验证最佳模型参数，又用来基于最小二乘误差来验证模型的泛化能力；
	b. 升级版——交叉验证法：
		i. 解决的问题：
			1) 对一个数据集最合适的多项式次数以及特征变量个数；
			2) 学习算法中正则化参数；
		ii. 即数据集分为训练集、验证集和测试集（6 ： 2 ： 2）；
	其中训练集用来得出不同多项式次数下特征变量的形式，验证机来验证最合适的多项式次数，最后使用测试集验证该模型的泛化能力；