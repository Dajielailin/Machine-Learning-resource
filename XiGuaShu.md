# 周志华-机器学习笔记/t1. 线性模型：/t	a. 对数几率回归：/t2. 极大似然法？/t3. 极大似然估计？/t4. 经典数值优化算法：/t	a. 梯度下降算法；/t	b. 牛顿法；/t5. 线性学习方法：/t	a. LDA：线性判别分析；/t		i. LDA朴素思想：/t			1) 给定训练样例集，设法将样例投影到一条直线上，使得同类样例的投影点尽可能接近，异类样例的投影点尽可能远离；/t			2) 在对新样本进行分类时，将其投影到这条直线上，根据投影点的位置判定样本的类别；/t		ii. LDA的目标：求得表示投影方向W的广义特征向量组成的d'维投影矩阵；/t		iii. LDA的实际效果就是：将样本的原有的d维属性降维到了投影矩阵的d'维空间；/t			/t	b. Fisher判别分析；/t	c. 拉格朗日乘子法；/t	d. 奇异值分解；/t	e. 数据集散度和协方差的关系？/t	f. 矩阵的迹？ Tr(.)/t	g. 广义特征值问题？/t6. 多分类学习：/t	a. 基本思路：/t		i. 先对问题拆分成多个二分类任务，然后为每个二分类任务训练一个分类器；/t		ii. 最后对多个分类器的预测结果进行集成以获取最终结果；/t	b. 经典拆分策略：/t		i. 一对一：OvO;/t		ii. 一对余：OvR;/t		iii. 多对多： MvM;/t			1) 纠错输出码；（Error Correcting Output Codes——ECOC）;/t			2) ECOC编码越长，纠错能力越强；相应地，所需训练的分类器越多，计算、存储开销也会增大；/t			3) 同等长度的编码（分类器个数相同），存在最优编码，即最优分类器种类，使得任意两个类别的间的编码距离最远（纠错能力更强）；——但是编码长度稍大时，难以有效确定最优编码。。。。NP问题？？？/t7. 模型评估与选择：/t	a. 训练误差与泛化误差；/t	b. 过拟合与欠拟合；/t8. 模型评估：/t	a. 数据集：/t		i. 训练集；/t		ii. 验证集；/t		iii. 测试集；/t	b. 划分数据集的方法：/t		i. 留出法；       //实际评估的模型所使用的训练集比数据集D小， 会引入估计偏差；/t		ii. 交叉验证法；  //实际评估的模型所使用的训练集比数据集D小， 会引入估计偏差；/t		iii. 自助法： 在数据集较小，难以有效划分训练和测试集时表现良好；但是自助法产生的数据集改变了初始数据集的分布，会引入估计偏差；/t9.   模型选择问题：/t	a. 学习算法；/t	b. 使用适合的参数配置；/t10. 性能度量：/t	a. 错误率与精度；/t	b. 查准率与查全率；/t		i. 查准率：预测为好瓜的样例中，真正为好瓜的概率； （P）/t		ii. 查全率：现实中的所有好瓜里，被预测为好瓜的概率；（R）/t		iii. PR图：/t			1) 根据学习器的预测结果对样例进行从大到小排序，按此顺序以每个样本为正例，每次计算出P和R/t			2) 以查全率为横轴，查准率为纵轴，做出PR图；/t		iv. 真正例率（TPR——True Positive rate）：预测为好瓜的样例中，实际为好瓜的概率；/t		v. 假正例率（FPR——False Positive rate）: 预测为坏瓜的样例中，实际为好瓜的概率；/t		vi. ROC图：/t			1) 方法同PR图，不过每次计算出的是TPR和FPR；/t			2) 然后以假正例率为横轴，真正例率为纵轴，做出ROC曲线；/t	c. 非均等代价：/t		i. 代价矩阵： COSTij——》表示将第i类样本预测为第j类样本的代价；/t		ii. 代价敏感错误率；/t11. 学习器性能的评估比较：/t	a. 统计假设检验：基于假设检验结果，我们可以推断出：/t		i. 若在测试集上观察到学习器A比B好，则A的泛化能力是否在统计意义上优于B；/t		ii. 且这个结论的把握有多大；/t	b. 统计假设检验方法：/t		i. 二项分布；/t		ii. 二项检验；/t		iii. t分布；/t		iv. X^2分布（卡方分布）；/t12. 解释学习算法泛化性能的重要工具：/t	a. 偏差；/t	b. 方差；/t	c. 噪声；/t	d. 偏差-方差分解；/t13. 机器学习方法——决策树：/t	a. 根节点——包含样本全集；/t	b. 若干内部节点——每个内部节点对应一个属性测试；该节点上的样本集合根据下一个属性测试的结果，再次划分到子结点中；/t	c. 若干叶节点——对应于决策结果；/t	d. 决策树的流程基本遵循"分而治之"的思想；/t	e. 决策树学习的关键——如何选择最优化分属性？/t		i. 信息增益准则——选择属性集中对样本集D划分获得信息增益最大的作为最优划分属性a——ID3决策树学习算法；——Gain(D, a)/t			1) 问题——信息增益准则对可取数目较多的属性有偏好，所以引入信息增益率；/t		ii. 信息增益率准则——C4.5决策树学习算法；/t14. 决策树：/t	a. 选择最优化分属性；/t	b. 信息增益：信息增益越大，则意味着使用属性a来对样本集合D划分所获得的“纯度提升越大”；/t	c. 信息熵：度量样本集合纯度最常用的一种指标；值越小，D的纯度越高；/t	d. CART： classification and regression tree;/t	e. 基尼指数：Gini(D)——反映了从数据集D中随机抽取两个样本，其类别标记不一致的概率；/t15. 剪枝处理：/t	a. 背景：决策树学习过程中，不断重复划分节点，导致决策树分支过多，以至于把训练集自身的一些特点当作所有数据都具有的一般性质而导致过拟合；/t	b. 剪枝是决策树学习算法对付“过拟合”的主要手段；/t	c. 基本策略：/t		i. 预剪枝：决策树生成过程中，对每个节点划分前进行估计(查准率)，若当前节点的划分不能带来决策树泛化性能的提升，则：/t			1) 停止划分节点；/t			2) 将当前节点标记为叶节点（叶节点的属性为当前节点样本子集里数目最多的属性）；/t		ii. 后剪枝：先生成完整的决策树，然后自底向上的对非叶节点进行考察，若/t			1) 将当前节点对应的子树替换为叶节点；/t			2) 验证集上可以看到修改后的决策树泛化性能得到了提升（验证集精度得到了提高）；/t		则将该子树替换为叶节点；/t	d. 预剪枝 VS 后剪枝：/t		i. 预剪枝：/t			1) 基于贪心的本质使得很多分支没有展开，减低了过拟合的风险，同时显著减少了决策树的训练时间和测试时间；/t			2) 但是决策树少了很多分支，带来了欠拟合的风险；/t		ii. 后剪枝：/t			1) 比预剪枝保留了更多的分支，欠拟合风险较小，泛化性能也由于预剪枝；/t			2) 但是需要自底向上对所有非叶节点尽心逐一考察，增加了大量训练样本时间；/t16. 支持向量机：/t	a. 背景：分类学习中最基本的想法是基于训练集D在样本空间中找到一个划分超平面，将不同的样本分开；/t	b. 若超平面(w, b)能将训练样本正确分类，即对于(x, y) 在D上，有　yi * (w*xi + b) >= 1，其中/t		i. 距离超平面最近的这几个训练样本使得上式的等号成立，它们被称为支持向量机；/t		ii. 两个异类支持向量到超平面的距离之和成为“间隔”；/t	c. 在训练集D= {xi， yi}上，欲找到具有“最大间隔”的划分超平面，问题也就等价于该超平面所对应的法向量w和原点位移b，/t		i. 该问题就是支持向量机的基本型——Support Vector Machine， 简称SVM；/t	d. 一般求解SVM模型的方法：/t		i. 使用拉格朗日乘子法得到SVM的对偶问题；/t		ii. 求解该对偶问题的高效算法有：/t			1) SMO——Sequential Minimal Optimization，基本思路如下：/t				a) 对拉格朗日算子参数初始化后，选取一对需要更新的变量ai和aj;/t				b) 固定ai和aj以外的参数，通过单变量的二次规划问题可以求出更新后的ai和aj;/t	e. 核函数——原始样本空间中Xi和Xj在映射到更高维特征空间里的内积可转换为它们通过核函数的计算结果；/t	f. 半正定矩阵？/t	g. 希尔伯特空间的特征空间？/t