# AiLearning之机器学习基础总结
## Logsitic回归：
### sigmoid阶跃函数：
### Tanh函数：sigmoid函数变形，且是0均值的：；
### 寻找最优参数的相关理论：
#### 梯度算子总是指向函数值增长最快的方向。即连续可微函数在某点P，沿在p点的梯度方向有最大的方向导数；
#### 梯度上升算法：与梯度下降算法的区别-目标函数是求最大似然函数：；——批处理学习算法
#### 梯度下降算法：目标函数为求最小损失函数：；
### 梯度上升算法改进：由于梯度上升算法每次更新回归系数时需要遍历整个数据集，当数据集较大时计算成本太高；
#### 随机梯度上升算法：即每次仅用一个样本点来更新回归系数；——在线学习算法；
## Logistic回归和最大熵模型：
### 1. 他们都属于对数线性模型。当类标签只有两个的时候，最大熵模型就是logistic回归模型；
## 多分类标签：逻辑回归也可用于多分类标签。
### 1. 对多分类标签，如果标签A中有a0,a1,a2,....,an个标签，则对每个标签ai，需要训练一个逻辑回归分类器；
### 2. 训练标签ai的逻辑回归分类器时，将标签ai看成一类，非ai标签看成另一类，这样就可以运用二值类逻辑回归分类算法；
### 3.测试数据的时候，将查询点套用在每个逻辑回归分类器中的Sigmoid 函数，取值最高的对应标签为查询点的标签。

## 支持向量机——监督学习算法
### 支持向量：就是离分割超平面最近的那些点；
### 机表示一种算法

## 神经网络 -DNN，CNN
### 神经网络非线性能力及原理：
#### 感知器和逻辑门——》空间分线性切分能力
### BP算法：也叫算法；
### SGD算法：梯度下降算法；
### 回归类问题选择损失函数：MSE-最小平方误差；
### 学习率：
### 常用的package工具： tersenflow， pytorch， keras

## 集成学习
### bagging——并行式集成学习方法的代表
#### 自主采样法——使得初始训练集中约有63.2%的数据流入采样集中；
#### 个体学习器之间无强依赖，可以并行生成；
#### 在对预测输出进行结合时，Bagging通常对分类任务使用简单投票法，对回归任务使用简单平均法；
### AdaBoosting——串行集成学习方法的代表
#### 
